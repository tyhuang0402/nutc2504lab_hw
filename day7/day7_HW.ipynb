{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d545d6",
   "metadata": {},
   "source": [
    "# Day 7 HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c99a3",
   "metadata": {},
   "source": [
    "## Define IDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "from docling.datamodel.pipeline_options import VlmPipelineOptions\n",
    "from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "\n",
    "vllm_hostname = \"ws-01.wade0426.me/v1\"\n",
    "model_name = \"allenai/olmOCR-2-7B-1025-FP8\"\n",
    "\n",
    "def process_rapidocr(source_file):\n",
    "    \"\"\"\n",
    "    Use RapidOCR to process file. Return a MD.\n",
    "    \"\"\"\n",
    "    pdf_options = PdfPipelineOptions(\n",
    "        do_ocr=True,\n",
    "    )\n",
    "\n",
    "    # 建立文件轉換器\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 轉換 PDF 文件\n",
    "    result = doc_converter.convert(source_file)\n",
    "\n",
    "    return result.document.export_to_markdown()\n",
    "\n",
    "\n",
    "def olmocr2_vlm_options(\n",
    "    model: str = \"allenai/olmOCR-2-7B-1025-FP8\",\n",
    "    hostname_and_port: str = \"https://ws-01.wade0426.me/v1/\",\n",
    "    prompt: str = \"Convert this page to markdown.\",\n",
    "    max_tokens: int = 4096,\n",
    "    temperature: float = 0.0,\n",
    "    api_key: str = \"\",) -> ApiVlmOptions:\n",
    "    \"\"\"\n",
    "    Options for olmOCR\n",
    "    \"\"\"\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "   \n",
    "    options = ApiVlmOptions(\n",
    "        url=f\"http://{hostname_and_port}/chat/completions\",\n",
    "        params=dict(\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "        ),\n",
    "        headers=headers,\n",
    "        prompt=prompt,\n",
    "        timeout=120,  # olmocr2 可能需要較長處理時間\n",
    "        scale=2.0,  # 圖片縮放比例\n",
    "        temperature=temperature,\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "    )\n",
    "    return options\n",
    "\n",
    "def process_olmocr(source_file):\n",
    "    \"\"\"\n",
    "    Use olmOCR to process file. Return a MD.\n",
    "    \"\"\"\n",
    "    # 配置 VLM pipeline 選項\n",
    "    pipeline_options = VlmPipelineOptions(\n",
    "        enable_remote_services=True  # 必須啟用以呼叫遠端 API\n",
    "    )\n",
    "\n",
    "    # 設定 olmocr2 的 VLM 選項\n",
    "    pipeline_options.vlm_options = olmocr2_vlm_options(\n",
    "        model=model_name,\n",
    "        hostname_and_port=vllm_hostname,\n",
    "        prompt=\"Convert this page to clean, readable markdown format.\",\n",
    "        temperature=0.0,  # olmocr2 建議使用較低的溫度\n",
    "    )\n",
    "\n",
    "    # 建立文件轉換器\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pipeline_options,\n",
    "                pipeline_cls=VlmPipeline,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    olm_result = doc_converter.convert(source=\"./HW/3.pdf\")\n",
    "\n",
    "    return olm_result.document.export_to_markdown()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d901cc",
   "metadata": {},
   "source": [
    "## Define Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2f52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'hwd7' created.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "\n",
    "collection_name = \"hwd7\"\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "if not client.collection_exists(collection_name=collection_name):\n",
    "    # If not, create the collection with specified parameters\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=100, distance=Distance.COSINE), # Adjust size and distance as needed\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' created.\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd179d3f",
   "metadata": {},
   "source": [
    "## Define LLMGuard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd40b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-11 16:31:26\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mInitialized classification model\u001b[0m \u001b[36mdevice\u001b[0m=\u001b[35mdevice(type='cpu')\u001b[0m \u001b[36mmodel\u001b[0m=\u001b[35mModel(path='protectai/deberta-v3-base-prompt-injection-v2', subfolder='', revision='89b085cd330414d3e7d9dd787870f315957e1e9f', onnx_path='ProtectAI/deberta-v3-base-prompt-injection-v2', onnx_revision='89b085cd330414d3e7d9dd787870f315957e1e9f', onnx_subfolder='onnx', onnx_filename='model.onnx', kwargs={}, pipeline_kwargs={'batch_size': 1, 'device': device(type='cpu'), 'return_token_type_ids': False, 'max_length': 512, 'truncation': True}, tokenizer_kwargs={})\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[*] 正在處理: ./HW/3.pdf\n",
      "============================================================\n",
      "\n",
      "[1/3] 提取文檔內容中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-11 16:31:26,619 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:26,650 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Winter4T2H\\source\\repo\\nutc2504lab_hw\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:26,651 [RapidOCR] main.py:53: Using C:\\Users\\Winter4T2H\\source\\repo\\nutc2504lab_hw\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:26,915 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:26,925 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Winter4T2H\\source\\repo\\nutc2504lab_hw\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:26,927 [RapidOCR] main.py:53: Using C:\\Users\\Winter4T2H\\source\\repo\\nutc2504lab_hw\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:27,178 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:27,280 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Winter4T2H\\source\\repo\\nutc2504lab_hw\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-11 16:31:27,283 [RapidOCR] main.py:53: Using C:\\Users\\Winter4T2H\\source\\repo\\nutc2504lab_hw\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 提取完成 (0 字元)\n",
      "\n",
      "[2/3] 掃描 Prompt Injection (分段檢測)...\n",
      "\n",
      "\n",
      "============================================================\n",
      "[3/3] 掃描結果\n",
      "============================================================\n",
      "總段落數: 0\n",
      "可疑段落數: 0\n",
      "最高風險分數: 0.00\n",
      "安全狀態: [PASS] 安全\n",
      "\n",
      "[INFO] 未發現可疑內容\n",
      "內容預覽 (前 300 字):\n",
      "------------------------------------------------------------\n",
      "...\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'safe': True,\n",
       " 'max_risk_score': 0.0,\n",
       " 'detections': 0,\n",
       " 'content': '',\n",
       " 'details': []}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_guard.input_scanners import PromptInjection, Anonymize\n",
    "from llm_guard.input_scanners.prompt_injection import MatchType\n",
    "\n",
    "class SimplePDFScanner:\n",
    "    def __init__(self):\n",
    "        self.converter = DocumentConverter()\n",
    "        # 降低閾值以提高敏感度\n",
    "        self.scanner = PromptInjection(threshold=0.75, match_type=MatchType.SENTENCE)\n",
    "   \n",
    "    def scan_pdf(self, pdf_path):\n",
    "        \"\"\"掃描 PDF 檔案中的 Prompt Injection 攻擊\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[*] 正在處理: {pdf_path}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "       \n",
    "        # Step 1: 提取 PDF 內容\n",
    "        print(\"[1/3] 提取文檔內容中...\")\n",
    "        result = self.converter.convert(pdf_path)\n",
    "        content = result.document.export_to_markdown()\n",
    "        print(f\"[OK] 提取完成 ({len(content)} 字元)\\n\")\n",
    "       \n",
    "        # Step 2: 分段掃描（提高檢測率）\n",
    "        print(\"[2/3] 掃描 Prompt Injection (分段檢測)...\\n\")\n",
    "        sections = self._split_content(content)\n",
    "        detections = []\n",
    "        max_risk = 0.0\n",
    "        total_sections = len(sections)\n",
    "       \n",
    "        for i, section in enumerate(sections, 1):\n",
    "            sanitized, is_safe, risk_score = self.scanner.scan(section)\n",
    "           \n",
    "            if not is_safe or risk_score > 0.3:\n",
    "                detections.append({\n",
    "                    'section': i,\n",
    "                    'risk_score': risk_score,\n",
    "                    'content': section[:200],\n",
    "                    'is_safe': is_safe\n",
    "                })\n",
    "                max_risk = max(max_risk, risk_score)\n",
    "                print(f\"  [!] 段落 {i}/{total_sections}: 風險分數 {risk_score:.2f} {'(未通過)' if not is_safe else '(警告)'}\")\n",
    "       \n",
    "        # Step 3: 顯示結果\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"[3/3] 掃描結果\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"總段落數: {total_sections}\")\n",
    "        print(f\"可疑段落數: {len(detections)}\")\n",
    "        print(f\"最高風險分數: {max_risk:.2f}\")\n",
    "        print(f\"安全狀態: {'[PASS] 安全' if len(detections) == 0 else '[FAIL] 偵測到風險'}\")\n",
    "       \n",
    "        if detections:\n",
    "            print(f\"\\n--- 可疑內容詳情 ---\")\n",
    "            for det in detections[:5]:  # 最多顯示 5 個\n",
    "                print(f\"\\n[段落 {det['section']}] 風險分數: {det['risk_score']:.2f}\")\n",
    "                print(f\"內容預覽: {det['content'][:150]}...\")\n",
    "        else:\n",
    "            print(f\"\\n[INFO] 未發現可疑內容\")\n",
    "            print(f\"內容預覽 (前 300 字):\")\n",
    "            print(\"-\"*60)\n",
    "            print(content[:300] + \"...\")\n",
    "       \n",
    "        print(f\"\\n{'='*60}\\n\")\n",
    "       \n",
    "        return {\n",
    "            'safe': len(detections) == 0,\n",
    "            'max_risk_score': max_risk,\n",
    "            'detections': len(detections),\n",
    "            'content': content,\n",
    "            'details': detections\n",
    "        }\n",
    "   \n",
    "    def _split_content(self, content, chunk_size=1000):\n",
    "        \"\"\"將內容分段以提高檢測率\"\"\"\n",
    "        # 先按段落分割\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        sections = []\n",
    "        current_section = \"\"\n",
    "       \n",
    "        for para in paragraphs:\n",
    "            if len(current_section) + len(para) > chunk_size:\n",
    "                if current_section:\n",
    "                    sections.append(current_section.strip())\n",
    "                current_section = para\n",
    "            else:\n",
    "                current_section += \"\\n\\n\" + para if current_section else para\n",
    "       \n",
    "        if current_section:\n",
    "            sections.append(current_section.strip())\n",
    "       \n",
    "        # 如果沒有段落，直接按字元切割\n",
    "        if len(sections) == 0:\n",
    "            sections = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "       \n",
    "        return [s for s in sections if len(s.strip()) > 50]\n",
    "\n",
    "scan_pdf = SimplePDFScanner()\n",
    "scan_pdf.scan_pdf(pdf_path=\"./HW/3.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
